{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqLnpvwnZjxu",
        "outputId": "82ac7085-55ac-4772-b3f1-7e8f26e56189"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK data files (only need to run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzMaF5A1aajo",
        "outputId": "708867f3-2fff-4741-ead0-5e2f699a6602"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    return words\n",
        "\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    # Preprocess the text\n",
        "    words = preprocess_text(text)\n",
        "\n",
        "    # Join words back to a single string for TF-IDF processing\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    # Vectorize the text using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([processed_text])\n",
        "\n",
        "    # Extract the TF-IDF scores and feature names\n",
        "    scores = tfidf_matrix.toarray()[0]\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get the top keywords based on TF-IDF scores\n",
        "    keyword_indices = scores.argsort()[-num_keywords:][::-1]\n",
        "    keywords = [feature_names[i] for i in keyword_indices]\n",
        "\n",
        "    return keywords\n"
      ],
      "metadata": {
        "id": "OtSoFT92aeMc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text (you can replace this with any article or blog text)\n",
        "text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
        "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving\".\n",
        "\"\"\"\n",
        "\n",
        "# Extract keywords\n",
        "keywords = extract_keywords(text, num_keywords=5)\n",
        "print(\"Keywords:\", keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0qNFhSRahqz",
        "outputId": "4f96acfa-a0b1-4c1d-ec92-35898fdea5d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords: ['intelligence', 'machines', 'ai', 'humans', 'artificial']\n"
          ]
        }
      ]
    }
  ]
}